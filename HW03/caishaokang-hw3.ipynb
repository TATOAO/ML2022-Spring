{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# If you want to access the version you have already modified, click \"Edit\"\n# If you want to access the original sample code, click \"...\", then click \"Copy & Edit Notebook\"","metadata":{}},{"cell_type":"code","source":"_exp_name = \"sample\"","metadata":{"papermill":{"duration":0.0189,"end_time":"2022-02-23T10:03:06.279758","exception":false,"start_time":"2022-02-23T10:03:06.260858","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-28T14:40:57.256513Z","iopub.execute_input":"2022-10-28T14:40:57.257154Z","iopub.status.idle":"2022-10-28T14:40:57.267232Z","shell.execute_reply.started":"2022-10-28T14:40:57.257074Z","shell.execute_reply":"2022-10-28T14:40:57.266496Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n# Import necessary packages.\nimport numpy as np\nimport torch\nimport os\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom PIL import Image\n# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\nfrom torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\nfrom torchvision.datasets import DatasetFolder, VisionDataset\n\n# This is for the progress bar.\nfrom tqdm.auto import tqdm\nimport random","metadata":{"papermill":{"duration":1.654263,"end_time":"2022-02-23T10:03:07.947242","exception":false,"start_time":"2022-02-23T10:03:06.292979","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-28T14:40:57.272572Z","iopub.execute_input":"2022-10-28T14:40:57.273407Z","iopub.status.idle":"2022-10-28T14:40:58.031539Z","shell.execute_reply.started":"2022-10-28T14:40:57.273369Z","shell.execute_reply":"2022-10-28T14:40:58.030713Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"myseed = 6666  # set a random seed for reproducibility\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(myseed)\ntorch.manual_seed(myseed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(myseed)","metadata":{"papermill":{"duration":0.078771,"end_time":"2022-02-23T10:03:08.039428","exception":false,"start_time":"2022-02-23T10:03:07.960657","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-28T14:40:58.033006Z","iopub.execute_input":"2022-10-28T14:40:58.034081Z","iopub.status.idle":"2022-10-28T14:40:58.112032Z","shell.execute_reply.started":"2022-10-28T14:40:58.034046Z","shell.execute_reply":"2022-10-28T14:40:58.111245Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## **Transforms**\nTorchvision provides lots of useful utilities for image preprocessing, data wrapping as well as data augmentation.\n\nPlease refer to PyTorch official website for details about different transforms.","metadata":{"papermill":{"duration":0.01289,"end_time":"2022-02-23T10:03:08.065357","exception":false,"start_time":"2022-02-23T10:03:08.052467","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Normally, We don't need augmentations in testing and validation.\n# All we need here is to resize the PIL image and transform it into Tensor.\ntest_tfm = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n])\n\n# However, it is also possible to use augmentation in the testing phase.\n# You may use train_tfm to produce a variety of images and then test using ensemble methods\ntrain_tfm = transforms.Compose([\n    # Resize the image into a fixed shape (height = width = 128)\n    #transforms.CenterCrop()\n    transforms.RandomResizedCrop((128, 128), scale=(0.7, 1.0)),\n    #transforms.AutoAugment(transforms.AutoAugmentPolicy.IMAGENET),\n    #transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    transforms.RandomHorizontalFlip(0.5),\n    transforms.RandomVerticalFlip(0.5),\n    transforms.RandomRotation(180),\n    transforms.RandomAffine(30),\n    #transforms.RandomInvert(p=0.2),\n    #transforms.RandomPosterize(bits=2),\n    #transforms.RandomSolarize(threshold=192.0, p=0.2),\n    #transforms.RandomEqualize(p=0.2),\n    transforms.RandomGrayscale(p=0.2),\n    transforms.ToTensor(),\n    #transforms.RandomApply(torch.nn.ModuleList([]))\n    # You may add some transforms here.\n    # ToTensor() should be the last one of the transforms.\n])\n","metadata":{"papermill":{"duration":0.021406,"end_time":"2022-02-23T10:03:08.099437","exception":false,"start_time":"2022-02-23T10:03:08.078031","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-28T14:40:58.114495Z","iopub.execute_input":"2022-10-28T14:40:58.114899Z","iopub.status.idle":"2022-10-28T14:40:58.121912Z","shell.execute_reply.started":"2022-10-28T14:40:58.114856Z","shell.execute_reply":"2022-10-28T14:40:58.121213Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## **Datasets**\nThe data is labelled by the name, so we load images and label while calling '__getitem__'","metadata":{"papermill":{"duration":0.012739,"end_time":"2022-02-23T10:03:08.125181","exception":false,"start_time":"2022-02-23T10:03:08.112442","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class FoodDataset(Dataset):\n\n    def __init__(self, path=None, tfm=test_tfm, files=None):\n        super(FoodDataset).__init__()\n        self.path = path\n        if files is not None:\n            self.files = files\n        else:\n            self.files = sorted([os.path.join(path, x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n\n        print(f\"One sample\", self.files[0])\n        self.transform = tfm\n  \n    def __len__(self):\n        return len(self.files)\n#         return 128\n  \n    def __getitem__(self,idx):\n        fname = self.files[idx]\n        im = Image.open(fname)\n        im = self.transform(im)\n        #im = self.data[idx]\n        try:\n            label = int(fname.split(\"/\")[-1].split(\"_\")[0])\n        except:\n            label = -1 # test has no label\n        return im,label\n\n","metadata":{"papermill":{"duration":0.023022,"end_time":"2022-02-23T10:03:08.160912","exception":false,"start_time":"2022-02-23T10:03:08.13789","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-28T14:40:58.123368Z","iopub.execute_input":"2022-10-28T14:40:58.123818Z","iopub.status.idle":"2022-10-28T14:40:58.133086Z","shell.execute_reply.started":"2022-10-28T14:40:58.123780Z","shell.execute_reply":"2022-10-28T14:40:58.132290Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class Residual_Block(nn.Module):\n    def __init__(self, ic, oc, stride=1):\n        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n        super().__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(ic, oc, kernel_size=3, stride=stride, padding=1),\n            nn.BatchNorm2d(oc),\n            nn.ReLU(inplace=True)\n        )\n        \n        self.conv2 = nn.Sequential(\n            nn.Conv2d(oc, oc, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(oc),\n        )\n        \n        self.relu = nn.ReLU(inplace=True)\n    \n        self.downsample = None\n        if stride != 1 or (ic != oc):\n            self.downsample = nn.Sequential(\n                nn.Conv2d(ic, oc, kernel_size=1, stride=stride),\n                nn.BatchNorm2d(oc),\n            )\n        \n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.conv2(out)\n        \n        if self.downsample:\n            residual = self.downsample(x)\n            \n        out += residual\n        return self.relu(out)\n        \nclass Classifier(nn.Module):\n    def __init__(self, block, num_layers, num_classes=11):\n        super().__init__()\n        self.preconv = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n        )\n        \n        self.layer0 = self.make_residual(block, 32, 64,  num_layers[0], stride=2)\n        self.layer1 = self.make_residual(block, 64, 128, num_layers[1], stride=2)\n        self.layer2 = self.make_residual(block, 128, 256, num_layers[2], stride=2)\n        self.layer3 = self.make_residual(block, 256, 512, num_layers[3], stride=2)\n        \n        #self.avgpool = nn.AvgPool2d(2)\n        \n        self.fc = nn.Sequential(            \n            nn.Dropout(0.4),\n            nn.Linear(512*4*4, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.2),\n            nn.Linear(512, 11),\n        )\n        \n        \n    def make_residual(self, block, ic, oc, num_layer, stride=1):\n        layers = []\n        layers.append(block(ic, oc, stride))\n        for i in range(1, num_layer):\n            layers.append(block(oc, oc))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        # [3, 128, 128]\n        out = self.preconv(x)  # [32, 64, 64]\n        out = self.layer0(out) # [64, 32, 32]\n        out = self.layer1(out) # [128, 16, 16]\n        out = self.layer2(out) # [256, 8, 8]\n        out = self.layer3(out) # [512, 4, 4]\n        #out = self.avgpool(out) # [512, 2, 2]\n        out = self.fc(out.view(out.size(0), -1)) \n        return out","metadata":{"papermill":{"duration":0.0258,"end_time":"2022-02-23T10:03:08.199437","exception":false,"start_time":"2022-02-23T10:03:08.173637","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-28T14:40:58.134736Z","iopub.execute_input":"2022-10-28T14:40:58.135197Z","iopub.status.idle":"2022-10-28T14:40:58.152758Z","shell.execute_reply.started":"2022-10-28T14:40:58.135161Z","shell.execute_reply":"2022-10-28T14:40:58.151961Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\nfrom torch.autograd import Variable\n\nclass FocalLoss(nn.Module):\n    def __init__(self, class_num, alpha=None, gamma=2, size_average=True):\n        super().__init__()\n        if alpha is None:\n            self.alpha = Variable(torch.ones(class_num, 1))\n        else:\n            if isinstance(alpha, Variable):\n                self.alpha = alpha\n            else:\n                self.alpha = Variable(alpha)\n        self.gamma = gamma\n        self.class_num = class_num\n        self.size_average = size_average\n        \n    def forward(self, inputs, targets):\n        N = inputs.size(0)\n        C = inputs.size(1)\n        P = F.softmax(inputs, dim=1)\n        \n        class_mask = inputs.data.new(N, C).fill_(0)\n        class_mask = Variable(class_mask)\n        ids = targets.view(-1, 1)\n        class_mask.scatter_(1, ids.data, 1.)\n        \n        if inputs.is_cuda and not self.alpha.is_cuda:\n            self.alpha = self.alpha.cuda()\n        alpha = self.alpha[ids.data.view(-1)]\n        probs = (P*class_mask).sum(1).view(-1, 1)\n        \n        log_p = probs.log()\n        \n        batch_loss = -alpha*(torch.pow((1-probs), self.gamma))*log_p\n        \n        if self.size_average:\n            loss = batch_loss.mean()\n        else:\n            loss = batch_loss.sum()\n            \n        return loss","metadata":{"execution":{"iopub.status.busy":"2022-10-28T14:40:58.154123Z","iopub.execute_input":"2022-10-28T14:40:58.154603Z","iopub.status.idle":"2022-10-28T14:40:58.167144Z","shell.execute_reply.started":"2022-10-28T14:40:58.154568Z","shell.execute_reply":"2022-10-28T14:40:58.166624Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# super params\nn_folds = 4\nbatch_size = 128\nalpha = torch.Tensor([1, 2.3, 0.66, 1, 1.1, 0.75, 2.3, 3.5, 1.1, 0.66, 1.4])\n# The number of training epochs and patience.\nn_epochs = 200\npatience = 30  # If no improvement in 'patience' epochs, early stop\nnum_layers = [2, 4, 3, 1]  # residual number layers","metadata":{"execution":{"iopub.status.busy":"2022-10-28T14:40:58.168418Z","iopub.execute_input":"2022-10-28T14:40:58.168860Z","iopub.status.idle":"2022-10-28T14:40:58.185564Z","shell.execute_reply.started":"2022-10-28T14:40:58.168819Z","shell.execute_reply":"2022-10-28T14:40:58.184810Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# data\n_dataset_dir = \"../input/ml2022spring-hw3b/food11\"\ntrain_data_path = os.path.join(_dataset_dir, \"training\")\nvalid_data_path = os.path.join(_dataset_dir, \"validation\")\n\ntrain_data_files = [os.path.join(train_data_path, x) for x in os.listdir(train_data_path) if x.endswith(\".jpg\")]\nvalid_data_files = [os.path.join(valid_data_path, x) for x in os.listdir(valid_data_path) if x.endswith(\".jpg\")]\n\ntotal_data_files = train_data_files + valid_data_files","metadata":{"papermill":{"duration":0.054295,"end_time":"2022-02-23T10:03:08.266338","exception":false,"start_time":"2022-02-23T10:03:08.212043","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-28T14:40:58.187033Z","iopub.execute_input":"2022-10-28T14:40:58.187550Z","iopub.status.idle":"2022-10-28T14:40:59.071416Z","shell.execute_reply.started":"2022-10-28T14:40:58.187515Z","shell.execute_reply":"2022-10-28T14:40:59.070408Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# \"cuda\" only when GPUs are available.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nfor i_fold in range(n_folds):\n    if i_fold == 0 or i_fold == 1 or i_fold == 3:\n        continue\n    # Initialize a model, and put it on the device specified.\n    model = Classifier(Residual_Block, num_layers).to(device)\n\n    # For the classification task, we use cross-entropy as the measurement of performance.\n    criterion = FocalLoss(11, alpha=alpha)\n\n    # Initialize optimizer, you may fine-tune some hyperparameters such as learning rate on your own.\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=16, T_mult=1)\n    # Initialize trackers, these are not parameters and should not be changed\n    stale = 0\n    best_acc = 0\n\n    total_num = int(len(total_data_files)/n_folds)\n    print(\"total_num:\", total_num)\n    valid_data_files = total_data_files[i_fold * total_num: (i_fold + 1) * total_num]\n    train_data_files = total_data_files[:i_fold * total_num] + total_data_files[(i_fold + 1) * total_num:]\n    train_set = FoodDataset(tfm=train_tfm, files=train_data_files)\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n    valid_set = FoodDataset(tfm=test_tfm, files=valid_data_files)\n    valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n\n    for epoch in range(n_epochs):\n\n        # ---------- Training ----------\n        # Make sure the model is in train mode before training.\n        model.train()\n\n        # These are used to record information in training.\n        train_loss = []\n        train_accs = []\n\n        pbar = tqdm(train_loader)\n        pbar.set_description(f'T: {epoch + 1:03d}/{n_epochs:03d}')\n        # for batch in tqdm(train_loader):\n        lr = optimizer.param_groups[0][\"lr\"]\n        for batch in pbar:\n            # A batch consists of image data and corresponding labels.\n            imgs, labels = batch\n            # imgs = imgs.half()\n            # print(imgs.shape,labels.shape)\n\n            # Forward the data. (Make sure data and model are on the same device.)\n            logits = model(imgs.to(device))\n            # Calculate the cross-entropy loss.\n            # We don't need to apply softmax before computing cross-entropy as it is done automatically.\n            loss = criterion(logits, labels.to(device))\n\n            # Gradients stored in the parameters in the previous step should be cleared out first.\n            optimizer.zero_grad()\n\n            # Compute the gradients for parameters.\n            loss.backward()\n\n            # Clip the gradient norms for stable training.\n            grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n\n            # Update the parameters with computed gradients.\n            optimizer.step()\n            # Compute the accuracy for current batch.\n            acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n\n            # Record the loss and accuracy.\n            train_loss.append(loss.item())\n            train_accs.append(acc)\n\n            train_loss_value = sum(train_loss) / len(train_loss)\n            train_acc_value = sum(train_accs).item() / len(train_accs)\n            pbar.set_postfix({'fold': i_fold, 'lr': lr, 'b_loss': loss.item(), 'b_acc': acc.item(),\n                              'loss': train_loss_value, 'acc': train_acc_value})\n        scheduler.step()\n        # Print the information.\n        # print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n        # ---------- Validation ----------\n        # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n        model.eval()\n        # These are used to record information in validation.\n        valid_loss = []\n        valid_accs = []\n        # Iterate the validation set by batches.\n        for batch in tqdm(valid_loader):\n            # A batch consists of image data and corresponding labels.\n            imgs, labels = batch\n            # imgs = imgs.half()\n\n            # We don't need gradient in validation.\n            # Using torch.no_grad() accelerates the forward process.\n            with torch.no_grad():\n                logits = model(imgs.to(device))\n\n            # We can still compute the loss (but not the gradient).\n            loss = criterion(logits, labels.to(device))\n\n            # Compute the accuracy for current batch.\n            acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n            # Record the loss and accuracy.\n            valid_loss.append(loss.item())\n            valid_accs.append(acc)\n            # break\n\n        # The average loss and accuracy for entire validation set is the average of the recorded values.\n        valid_loss = sum(valid_loss) / len(valid_loss)\n        valid_acc = sum(valid_accs) / len(valid_accs)\n\n        # Print the information.\n        print(\n            f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}, fold = {i_fold}\")\n        # update logs\n        if valid_acc > best_acc:\n            with open(f\"./{_exp_name}_log.txt\", \"a\"):\n                print(\n                    f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f} -> best\")\n        else:\n            with open(f\"./{_exp_name}_log.txt\", \"a\"):\n                print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n\n        # save models\n        if valid_acc > best_acc:\n            print(f\"Best model found at epoch {epoch}, saving model\")\n            torch.save(model.state_dict(),\n                       f\"{_exp_name}_{i_fold}_best.ckpt\")  # only save best to prevent output memory exceed error\n            best_acc = valid_acc\n            stale = 0\n        else:\n            stale += 1\n            if stale > patience:\n                print(f\"No improvment {patience} consecutive epochs, early stopping\")\n                break","metadata":{"papermill":{"duration":32830.720158,"end_time":"2022-02-23T19:10:19.001001","exception":false,"start_time":"2022-02-23T10:03:08.280843","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-28T14:40:59.073132Z","iopub.execute_input":"2022-10-28T14:40:59.073674Z","iopub.status.idle":"2022-10-28T14:41:15.609658Z","shell.execute_reply.started":"2022-10-28T14:40:59.073635Z","shell.execute_reply":"2022-10-28T14:41:15.608747Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"total_num: 3324\nOne sample ../input/ml2022spring-hw3b/food11/training/2_603.jpg\nOne sample ../input/ml2022spring-hw3b/food11/training/8_248.jpg\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44a296baa8794cf39b6dcd51a792f8ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc811927a4164f66ae5c8b2b1afa4354"}},"metadata":{}},{"name":"stdout","text":"[ Valid | 001/001 ] loss = 2.00517, acc = 0.07812, fold = 0\n[ Valid | 001/001 ] loss = 2.00517, acc = 0.07812 -> best\nBest model found at epoch 0, saving model\ntotal_num: 3324\nOne sample ../input/ml2022spring-hw3b/food11/training/8_248.jpg\nOne sample ../input/ml2022spring-hw3b/food11/training/2_603.jpg\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0daec9aeb241412d94c2986f970c3915"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6ee157b39c2465caddd5e0e058c40ab"}},"metadata":{}},{"name":"stdout","text":"[ Valid | 001/001 ] loss = 2.13848, acc = 0.04688, fold = 1\n[ Valid | 001/001 ] loss = 2.13848, acc = 0.04688 -> best\nBest model found at epoch 0, saving model\n","output_type":"stream"}]},{"cell_type":"code","source":"# test_set = FoodDataset(os.path.join(_dataset_dir,\"test\"), tfm=test_tfm)\n# test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)","metadata":{"papermill":{"duration":0.493644,"end_time":"2022-02-23T19:10:19.985992","exception":false,"start_time":"2022-02-23T19:10:19.492348","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-28T14:41:15.611327Z","iopub.execute_input":"2022-10-28T14:41:15.611605Z","iopub.status.idle":"2022-10-28T14:41:16.325709Z","shell.execute_reply.started":"2022-10-28T14:41:15.611568Z","shell.execute_reply":"2022-10-28T14:41:16.324929Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"One sample ../input/ml2022spring-hw3b/food11/test/0001.jpg\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Testing and generate prediction CSV","metadata":{"papermill":{"duration":0.498773,"end_time":"2022-02-23T19:10:20.961802","exception":false,"start_time":"2022-02-23T19:10:20.463029","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# models = []\n# for i_fold in range(n_folds):\n#     model_best = Classifier(Residual_Block, num_layers).to(device)\n#     model_best.load_state_dict(torch.load(f\"{_exp_name}_{i_fold}_best.ckpt\"))\n#     model_best.eval()\n#     models.append(model_best)\n\n# prediction = []\n# with torch.no_grad():\n#     for data, _ in test_loader:\n#         test_preds = []\n#         for model_best in models:\n#             test_pred = model_best(data.to(device))\n#             test_preds.append(test_pred.cpu().data.numpy())\n#         test_preds = sum(test_preds)\n#         test_label = np.argmax(test_preds, axis=1)\n#         prediction += test_label.squeeze().tolist()\n","metadata":{"papermill":{"duration":49.157727,"end_time":"2022-02-23T19:11:10.61523","exception":false,"start_time":"2022-02-23T19:10:21.457503","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-28T14:41:16.327151Z","iopub.execute_input":"2022-10-28T14:41:16.327418Z","iopub.status.idle":"2022-10-28T14:41:16.332161Z","shell.execute_reply.started":"2022-10-28T14:41:16.327382Z","shell.execute_reply":"2022-10-28T14:41:16.331124Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#create test csv\n# def pad4(i):\n#     return \"0\"*(4-len(str(i)))+str(i)\n# df = pd.DataFrame()\n# df[\"Id\"] = [pad4(i) for i in range(1,len(test_set)+1)]\n# df[\"Category\"] = prediction\n# df.to_csv(\"submission.csv\",index = False)","metadata":{"papermill":{"duration":0.554276,"end_time":"2022-02-23T19:11:11.870035","exception":false,"start_time":"2022-02-23T19:11:11.315759","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-28T14:41:16.335472Z","iopub.execute_input":"2022-10-28T14:41:16.335810Z","iopub.status.idle":"2022-10-28T14:41:16.342281Z","shell.execute_reply.started":"2022-10-28T14:41:16.335770Z","shell.execute_reply":"2022-10-28T14:41:16.341303Z"},"trusted":true},"execution_count":13,"outputs":[]}]}