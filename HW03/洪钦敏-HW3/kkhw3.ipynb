{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# utility","metadata":{}},{"cell_type":"code","source":"# Import necessary packages.\nimport numpy as np\nimport pandas as pd\nimport torch\nimport os\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport torchvision.models as models\n# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\nfrom torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\nfrom torchvision.datasets import DatasetFolder, VisionDataset\n\n# This is for the progress bar.\nfrom tqdm.auto import tqdm\nimport random\n\n# \"cuda\" only when GPUs are available.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ndef same_seeds(seed):\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\n\n# Torchvision provides lots of useful utilities for image preprocessing\n# data wrapping as well as data augmentation.\n# Normally, We don't need augmentations in testing and validation.\n# All we need here is to resize the PIL image and transform it into Tensor.\ntest_tfm = transforms.Compose([\n    transforms.Resize((128, 128)),\n    \n    transforms.ToTensor(),\n])\n\n# However, it is also possible to use augmentation in the testing phase.\n# You may use train_tfm to produce a variety of images and then test using ensemble methods\ntrain_tfm = transforms.Compose([\n    # Resize the image into a fixed shape (height = width = 128)\n    transforms.Resize((128, 128)),\n    # You may add some transforms here.\n    # ToTensor() should be the last one of the transforms.\n    transforms.RandomRotation(180),\n    transforms.RandomHorizontalFlip(0.5),\n    transforms.RandomVerticalFlip(0.5),\n    transforms.ToTensor(),\n])\n\n# 生成多张图片的方法\ndef spec_transformer(method):\n    tfm = transforms.Compose([\n        transforms.Resize((128, 128)),\n        method,\n        transforms.ToTensor(),\n    ])\n    return tfm\n\n# 生成多张图片的方法 传入多个变换方法\ndef spec_transformers(methods):\n    tfm = transforms.Compose([\n        *[x for x in methods],\n        transforms.Resize((128, 128)),\n        transforms.ToTensor(),\n    ])\n    return tfm\n\n# 这个方法可以生成多张变换的图片\n#     fff = [spec_transformers([]),\n#            spec_transformers([transforms.RandomHorizontalFlip(1), transforms.RandomVerticalFlip(1)])]\n# x = spec_transformers([])(im)\n","metadata":{"execution":{"iopub.status.busy":"2022-11-05T09:20:03.322025Z","iopub.execute_input":"2022-11-05T09:20:03.322951Z","iopub.status.idle":"2022-11-05T09:20:05.259512Z","shell.execute_reply.started":"2022-11-05T09:20:03.322845Z","shell.execute_reply":"2022-11-05T09:20:05.258439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# parameters","metadata":{}},{"cell_type":"code","source":"_exp_name = \"main_aug\"\nbatch_size = 64\n_dataset_dir = \"/kaggle/input/ml2022spring-hw3b/food11\"\nmyseed = 76553366  # set a random seed for reproducibility\n# The number of training epochs and patience.\nn_epochs = 100\npatience = 20 # If no improvement in 'patience' epochs, early stop\nlearning_rate = 3*1e-4\nweight_decay = 1e-5\n# 结果加权的向量 注意长度必须能被batch_size整除 并且等于图片复制的数量\nensemble_weight = np.array([0.4, 0.2, 0.2, 0.2])\n# my_trans = [spec_transformers([transforms.RandAugment()]),\n#             spec_transformers([transforms.RandomRotation(180),\n#                                transforms.RandomResizedCrop((128, 128), scale=(0.5, 1.0), ratio=(0.8, 1.2)),\n#                                transforms.RandomAutocontrast(),\n#                                transforms.ColorJitter(brightness=(0.7, 1.3), contrast=(0.7, 1.3),\n#                                                       saturation=(0.7, 1.3)),\n#                                transforms.RandomPerspective(distortion_scale=np.random.rand())]),\n#             spec_transformers([transforms.RandomHorizontalFlip(),\n#                                transforms.RandomVerticalFlip(),\n#                                transforms.RandomResizedCrop((128, 128), scale=(0.5, 1.0), ratio=(0.8, 1.2)),\n#                                transforms.RandomAdjustSharpness(2 * np.random.rand()),\n#                                transforms.ColorJitter(brightness=(0.7, 1.3), contrast=(0.7, 1.3),\n#                                                       saturation=(0.7, 1.3))]),\n#             spec_transformers([transforms.RandomHorizontalFlip(),\n#                                transforms.RandomVerticalFlip(),\n#                                transforms.RandomAffine(degrees=30, translate=(0.1, 0.2), shear=(0, 30)),\n#                                transforms.RandomAdjustSharpness(2 * np.random.rand()),\n#                                transforms.ColorJitter(brightness=(0.7, 1.3), contrast=(0.7, 1.3),\n#                                                       saturation=(0.7, 1.3))])\n#             ]\n\nmy_trans = [spec_transformers([transforms.RandAugment(),transforms.RandAugment(),transforms.RandAugment()]),\n             spec_transformers([transforms.RandAugment(),transforms.RandomRotation(180),\n                             transforms.RandomResizedCrop((128, 128), scale=(0.5, 1.0), ratio=(0.8, 1.2)),\n                             transforms.RandomAutocontrast(),\n                             transforms.ColorJitter(brightness=(0.7, 1.3), contrast=(0.7, 1.3),\n                                                    saturation=(0.7, 1.3)),\n                             transforms.RandomPerspective(distortion_scale=np.random.rand())]),\n             spec_transformers([transforms.RandAugment(),transforms.RandomHorizontalFlip(),\n                             transforms.RandomVerticalFlip(),\n                             transforms.RandomResizedCrop((128, 128), scale=(0.5, 1.0), ratio=(0.8, 1.2)),\n                             transforms.RandomAdjustSharpness(2*np.random.rand()),\n                             transforms.ColorJitter(brightness=(0.7, 1.3), contrast=(0.7, 1.3),\n                                                    saturation=(0.7, 1.3))]),\n             spec_transformers([transforms.RandAugment(),transforms.RandomHorizontalFlip(),\n                             transforms.RandomVerticalFlip(),\n                             transforms.RandomAffine(degrees=30,translate=(0.1,0.2),shear=(0,30)),\n                             transforms.RandomAdjustSharpness(2*np.random.rand()),\n                             transforms.ColorJitter(brightness=(0.7, 1.3), contrast=(0.7, 1.3),\n                                                    saturation=(0.7, 1.3))])\n             ]\n\ntest_trans = [spec_transformers([])]+my_trans[1:]\n\nnum_of_transformers = len(my_trans)","metadata":{"execution":{"iopub.status.busy":"2022-11-05T09:20:09.265041Z","iopub.execute_input":"2022-11-05T09:20:09.265541Z","iopub.status.idle":"2022-11-05T09:20:09.282020Z","shell.execute_reply.started":"2022-11-05T09:20:09.265507Z","shell.execute_reply":"2022-11-05T09:20:09.280945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data load","metadata":{}},{"cell_type":"code","source":"class FoodDataset(Dataset):\n\n    def __init__(self,path,tfm=test_tfm,files = None):\n        super(FoodDataset).__init__()\n        self.path = path\n        self.files = sorted([os.path.join(path,x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n        if files != None:\n            self.files = files\n        print(f\"One {path} sample\",self.files[0])\n        self.transform = tfm\n  \n    def __len__(self):\n        return len(self.files)\n  \n    def __getitem__(self,idx):\n        fname = self.files[idx]\n        im = Image.open(fname)\n        im = self.transform(im)\n        #im = self.data[idx]\n        try:\n            label = int(fname.split(\"/\")[-1].split(\"_\")[0])\n        except:\n            label = -1 # test has no label\n        return im,label\n    \n# 重写数据载入  tfms 为transformers的集合\nclass MultiFoodDataset(Dataset):\n\n    def __init__(self, path, tfms, files=None):\n        super(FoodDataset).__init__()\n        self.path = path\n        self.files = sorted([os.path.join(path, x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n        if files is not None:\n            self.files = files\n        print(f\"One {path} sample\", self.files[0])\n        self.transforms = tfms\n        # 图片重复次数\n        self.pic_duplicate_times = len(tfms)\n\n    def __len__(self):\n        return self.pic_duplicate_times * len(self.files)\n\n    def __getitem__(self, idx):\n        fname = self.files[idx // self.pic_duplicate_times]\n        im = Image.open(fname)\n        im = self.transforms[idx % self.pic_duplicate_times](im)\n        try:\n            label = int(fname.split(\"/\")[-1].split(\"_\")[0])\n        except:\n            label = -1  # test has no label\n        return im, label\n    \n# Construct datasets.\n# The argument \"loader\" tells how torchvision reads the data.\nsame_seeds(myseed)\ntrain_set = MultiFoodDataset(os.path.join(_dataset_dir,\"training\"), tfms=my_trans)\n# train_set = FoodDataset(os.path.join(_dataset_dir,\"training\"), tfm=train_tfm)\nprint('训练数据集',len(train_set))\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\nvalid_set = FoodDataset(os.path.join(_dataset_dir,\"validation\"), tfm=test_tfm)\nvalid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True) \ndel train_set,valid_set","metadata":{"execution":{"iopub.status.busy":"2022-11-05T09:20:17.932872Z","iopub.execute_input":"2022-11-05T09:20:17.933234Z","iopub.status.idle":"2022-11-05T09:20:18.175709Z","shell.execute_reply.started":"2022-11-05T09:20:17.933205Z","shell.execute_reply":"2022-11-05T09:20:18.174636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n        # input 維度 [3, 128, 128]\n        self.cnn = nn.Sequential(\n            nn.Conv2d(3, 64, 3, 1, 1),  # [64, 128, 128]\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2, 0),      # [64, 64, 64]\n\n            nn.Conv2d(64, 128, 3, 1, 1), # [128, 64, 64]\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2, 0),      # [128, 32, 32]\n\n            nn.Conv2d(128, 256, 3, 1, 1), # [256, 32, 32]\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2, 0),      # [256, 16, 16]\n\n            nn.Conv2d(256, 512, 3, 1, 1), # [512, 16, 16]\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2, 0),       # [512, 8, 8]\n            \n            nn.Conv2d(512, 512, 3, 1, 1), # [512, 8, 8]\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2, 0),       # [512, 4, 4]\n        )\n        self.fc = nn.Sequential(\n            nn.Linear(512*4*4, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 11)\n        )\n\n    def forward(self, x):\n        out = self.cnn(x)\n        out = out.view(out.size()[0], -1)\n        return self.fc(out)\n    \nclass ResNet(nn.Module):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n        # input 維度 [3, 128, 128]\n        self.cnn = nn.Sequential(\n        )\n        self.fc = nn.Sequential(\n            nn.ReLU(),\n            nn.Linear(512, 11)\n        )\n\n    def forward(self, x):\n        out = self.cnn(x)\n        out = out.view(out.size()[0], -1)\n        return self.fc(out)","metadata":{"execution":{"iopub.status.busy":"2022-11-03T06:30:48.235982Z","iopub.execute_input":"2022-11-03T06:30:48.236412Z","iopub.status.idle":"2022-11-03T06:30:48.256401Z","shell.execute_reply.started":"2022-11-03T06:30:48.236376Z","shell.execute_reply":"2022-11-03T06:30:48.255310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"# Initialize a model, and put it on the device specified.\n# model = Classifier().to(device)\nmodel = models.resnet50(pretrained=False)\nmodel.conv1.kernel_size = (3,3)\nmodel.fc.out_features = 11\nmodel.load_state_dict(torch.load(f\"../input/model/aug_best.ckpt\"))\nmodel = model.to(device)\n\n# For the classification task, we use cross-entropy as the measurement of performance.\ncriterion = nn.CrossEntropyLoss()\n\n# Initialize optimizer, you may fine-tune some hyperparameters such as learning rate on your own.\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay) \n\n# Initialize trackers, these are not parameters and should not be changed\nstale = 0\nbest_acc = 0\n\nfor epoch in range(n_epochs):\n\n    # ---------- Training ----------\n    # Make sure the model is in train mode before training.\n    model.train()\n\n    # These are used to record information in training.\n    train_loss = []\n    train_accs = []\n\n    for batch in tqdm(train_loader):\n\n        # A batch consists of image data and corresponding labels.\n        imgs, labels = batch\n        #imgs = imgs.half()\n        #print(imgs.shape,labels.shape)\n\n        # Forward the data. (Make sure data and model are on the same device.)\n        logits = model(imgs.to(device))\n\n        # Calculate the cross-entropy loss.\n        # We don't need to apply softmax before computing cross-entropy as it is done automatically.\n        loss = criterion(logits, labels.to(device))\n\n        # Gradients stored in the parameters in the previous step should be cleared out first.\n        optimizer.zero_grad()\n\n        # Compute the gradients for parameters.\n        loss.backward()\n\n        # Clip the gradient norms for stable training.\n        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n\n        # Update the parameters with computed gradients.\n        optimizer.step()\n\n        # Compute the accuracy for current batch.\n        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n\n        # Record the loss and accuracy.\n        train_loss.append(loss.item())\n        train_accs.append(acc)\n        \n    train_loss = sum(train_loss) / len(train_loss)\n    train_acc = sum(train_accs) / len(train_accs)\n\n    # Print the information.\n    print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n\n    # ---------- Validation ----------\n    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n    model.eval()\n\n    # These are used to record information in validation.\n    valid_loss = []\n    valid_accs = []\n\n    # Iterate the validation set by batches.\n    for batch in tqdm(valid_loader):\n\n        # A batch consists of image data and corresponding labels.\n        imgs, labels = batch\n        #imgs = imgs.half()\n\n        # We don't need gradient in validation.\n        # Using torch.no_grad() accelerates the forward process.\n        with torch.no_grad():\n            logits = model(imgs.to(device))\n\n        # We can still compute the loss (but not the gradient).\n        loss = criterion(logits, labels.to(device))\n\n        # Compute the accuracy for current batch.\n        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n\n        # Record the loss and accuracy.\n        valid_loss.append(loss.item())\n        valid_accs.append(acc)\n        #break\n\n    # The average loss and accuracy for entire validation set is the average of the recorded values.\n    valid_loss = sum(valid_loss) / len(valid_loss)\n    valid_acc = sum(valid_accs) / len(valid_accs)\n\n    # Print the information.\n    print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n\n\n    # update logs\n    if valid_acc > best_acc:\n        with open(f\"./{_exp_name}_log.txt\",\"a\"):\n            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f} -> best\")\n    else:\n        with open(f\"./{_exp_name}_log.txt\",\"a\"):\n            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n\n\n    # save models\n    if valid_acc > best_acc:\n        print(f\"Best model found at epoch {epoch}, saving model\")\n        torch.save(model.state_dict(), f\"{_exp_name}_best.ckpt\") # only save best to prevent output memory exceed error\n        best_acc = valid_acc\n        stale = 0\n    else:\n        stale += 1\n        if stale > patience:\n            print(f\"No improvment {patience} consecutive epochs, early stopping\")\n            break","metadata":{"execution":{"iopub.status.busy":"2022-11-03T06:30:58.458713Z","iopub.execute_input":"2022-11-03T06:30:58.459123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Result","metadata":{}},{"cell_type":"code","source":"# del train_loader,valid_loader\n# test_set = FoodDataset(os.path.join(_dataset_dir,\"test\"), tfm=test_tfm)\n# test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n\n# model_best = Classifier().to(device)\n# model_best.load_state_dict(torch.load(f\"{_exp_name}_best.ckpt\"))\n# model_best.eval()\n# prediction = []\n# with torch.no_grad():\n#     for data,_ in test_loader:\n#         test_pred = model_best(data.to(device))\n#         test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)\n#         prediction += test_label.squeeze().tolist()\n        \n# #create test csv\n# def pad4(i):\n#     return \"0\"*(4-len(str(i)))+str(i)\n# df = pd.DataFrame()\n# df[\"Id\"] = [pad4(i) for i in range(1,len(test_set)+1)]\n# df[\"Category\"] = prediction\n# df.to_csv(\"submission.csv\",index = False)        ","metadata":{"execution":{"iopub.status.busy":"2022-10-30T10:26:21.380561Z","iopub.status.idle":"2022-10-30T10:26:21.381473Z","shell.execute_reply.started":"2022-10-30T10:26:21.381068Z","shell.execute_reply":"2022-10-30T10:26:21.381094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Result with Ensemble","metadata":{}},{"cell_type":"code","source":"test_set = MultiFoodDataset(os.path.join(_dataset_dir, \"test\"), tfms=test_trans)\ntest_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n\n\n# 这里要加 , map_location='cpu'  因为GPU训练的模型转CPU会不兼容  这里用的参数 为kaggle 前几个版本的初始sample\nmodel_best = models.resnet50(pretrained=False)\nmodel_best.conv1.kernel_size = (3,3)\nmodel_best.fc.out_features = 11\nmodel_best.load_state_dict(torch.load(f\"../input/model/main_aug_best.ckpt\"))\n# model_best.load_state_dict(torch.load(f\"{_exp_name}_best.ckpt\"))\n\nmodel_best = model_best.to(device)\n\nmodel_best.eval()\nprediction = []\n\n\nwith torch.no_grad():\n    for data, _ in test_loader:\n        # shape 为batch_size * num_of_classes\n        test_pred = model_best(data.to(device))\n#         print(test_pred.shape)\n        batch_len = test_pred.shape[0]\n        # shape 为batch_size\n        oh = test_pred.cpu().data.numpy().reshape(batch_len // num_of_transformers, num_of_transformers, 1000)\n        oh = np.dot(ensemble_weight, oh)\n        test_label = np.argmax(oh, axis=1)\n        prediction += test_label.squeeze().tolist()\n\n\n# create test csv\ndef pad4(i):\n    return \"0\" * (4 - len(str(i))) + str(i)\n\n\ndf = pd.DataFrame()\ndf[\"Id\"] = [pad4(i) for i in range(1, len(test_set) // num_of_transformers + 1)]\ndf[\"Category\"] = prediction\ndf.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-11-05T09:20:53.229586Z","iopub.execute_input":"2022-11-05T09:20:53.230667Z","iopub.status.idle":"2022-11-05T09:25:28.175257Z","shell.execute_reply.started":"2022-11-05T09:20:53.230604Z","shell.execute_reply":"2022-11-05T09:25:28.173942Z"},"trusted":true},"execution_count":null,"outputs":[]}]}