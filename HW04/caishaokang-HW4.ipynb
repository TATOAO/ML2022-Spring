{"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"hw04.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Task description\n- Classify the speakers of given features.\n- Main goal: Learn how to use transformer.\n- Baselines:\n  - Easy: Run sample code and know how to use transformer.\n  - Medium: Know how to adjust parameters of transformer.\n  - Strong: Construct [conformer](https://arxiv.org/abs/2005.08100) which is a variety of transformer. \n  - Boss: Implement [Self-Attention Pooling](https://arxiv.org/pdf/2008.01077v1.pdf) & [Additive Margin Softmax](https://arxiv.org/pdf/1801.05599.pdf) to further boost the performance.\n\n- Other links\n  - Kaggle: [link](https://www.kaggle.com/t/ac77388c90204a4c8daebeddd40ff916)\n  - Slide: [link](https://docs.google.com/presentation/d/1HLAj7UUIjZOycDe7DaVLSwJfXVd3bXPOyzSb6Zk3hYU/edit?usp=sharing)\n  - Data: [link](https://drive.google.com/drive/folders/1vI1kuLB-q1VilIftiwnPOCAeOOFfBZge?usp=sharing)\n\n# Download dataset\n- Data is [here](https://drive.google.com/drive/folders/1vI1kuLB-q1VilIftiwnPOCAeOOFfBZge?usp=sharing)","metadata":{"id":"C_jdZ5vHJ4A9"}},{"cell_type":"code","source":"!wget https://github.com/MachineLearningHW/ML_HW4_Dataset/releases/latest/download/Dataset.tar.gz.partaa\n!wget https://github.com/MachineLearningHW/ML_HW4_Dataset/releases/latest/download/Dataset.tar.gz.partab\n!wget https://github.com/MachineLearningHW/ML_HW4_Dataset/releases/latest/download/Dataset.tar.gz.partac\n!wget https://github.com/MachineLearningHW/ML_HW4_Dataset/releases/latest/download/Dataset.tar.gz.partad\n\n!cat Dataset.tar.gz.part* > Dataset.tar.gz\n\n# unzip the file\n!tar zxvf Dataset.tar.gz","metadata":{"id":"LhLNWB-AK2Z5","outputId":"d2481dbd-f069-4562-8544-cf4871734cc1","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2022-10-12T07:24:04.933074Z","iopub.execute_input":"2022-10-12T07:24:04.933466Z","iopub.status.idle":"2022-10-12T07:24:11.200228Z","shell.execute_reply.started":"2022-10-12T07:24:04.933435Z","shell.execute_reply":"2022-10-12T07:24:11.199061Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"--2022-10-12 07:24:05--  https://github.com/MachineLearningHW/ML_HW4_Dataset/releases/latest/download/Dataset.tar.gz.partaa\nResolving github.com (github.com)... 140.82.112.4\nConnecting to github.com (github.com)|140.82.112.4|:443... connected.\nHTTP request sent, awaiting response... 404 Not Found\n2022-10-12 07:24:05 ERROR 404: Not Found.\n\n--2022-10-12 07:24:06--  https://github.com/MachineLearningHW/ML_HW4_Dataset/releases/latest/download/Dataset.tar.gz.partab\nResolving github.com (github.com)... 140.82.112.4\nConnecting to github.com (github.com)|140.82.112.4|:443... connected.\nHTTP request sent, awaiting response... 404 Not Found\n2022-10-12 07:24:06 ERROR 404: Not Found.\n\n--2022-10-12 07:24:07--  https://github.com/MachineLearningHW/ML_HW4_Dataset/releases/latest/download/Dataset.tar.gz.partac\nResolving github.com (github.com)... 140.82.112.4\nConnecting to github.com (github.com)|140.82.112.4|:443... connected.\nHTTP request sent, awaiting response... 404 Not Found\n2022-10-12 07:24:08 ERROR 404: Not Found.\n\n--2022-10-12 07:24:09--  https://github.com/MachineLearningHW/ML_HW4_Dataset/releases/latest/download/Dataset.tar.gz.partad\nResolving github.com (github.com)... 140.82.112.4\nConnecting to github.com (github.com)|140.82.112.4|:443... connected.\nHTTP request sent, awaiting response... 404 Not Found\n2022-10-12 07:24:09 ERROR 404: Not Found.\n\ncat: 'Dataset.tar.gz.part*': No such file or directory\n\ngzip: stdin: unexpected end of file\ntar: Child returned status 1\ntar: Error is not recoverable: exiting now\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Fix Random Seed","metadata":{"id":"ENWVAUDVJtVY"}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport random\n\ndef set_seed(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\nset_seed(87)","metadata":{"id":"E6burzCXIyuA","execution":{"iopub.status.busy":"2022-10-12T07:24:11.202867Z","iopub.execute_input":"2022-10-12T07:24:11.203587Z","iopub.status.idle":"2022-10-12T07:24:11.210419Z","shell.execute_reply.started":"2022-10-12T07:24:11.203543Z","shell.execute_reply":"2022-10-12T07:24:11.209362Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"# Data\n\n## Dataset\n- Original dataset is [Voxceleb2](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html).\n- The [license](https://creativecommons.org/licenses/by/4.0/) and [complete version](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/files/license.txt) of Voxceleb2.\n- We randomly select 600 speakers from Voxceleb2.\n- Then preprocess the raw waveforms into mel-spectrograms.\n\n- Args:\n  - data_dir: The path to the data directory.\n  - metadata_path: The path to the metadata.\n  - segment_len: The length of audio segment for training. \n- The architecture of data directory \\\\\n  - data directory \\\\\n  |---- metadata.json \\\\\n  |---- testdata.json \\\\\n  |---- mapping.json \\\\\n  |---- uttr-{random string}.pt \\\\\n\n- The information in metadata\n  - \"n_mels\": The dimention of mel-spectrogram.\n  - \"speakers\": A dictionary. \n    - Key: speaker ids.\n    - value: \"feature_path\" and \"mel_len\"\n\n\nFor efficiency, we segment the mel-spectrograms into segments in the traing step.","metadata":{"id":"k7dVbxW2LASN"}},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport random\nfrom pathlib import Path\nfrom torch.utils.data import Dataset\nfrom torch.nn.utils.rnn import pad_sequence\n \n \nclass myDataset(Dataset):\n\tdef __init__(self, data_dir, segment_len=128):\n\t\tself.data_dir = data_dir\n\t\tself.segment_len = segment_len\n\t\n\t\t# Load the mapping from speaker neme to their corresponding id. \n\t\tmapping_path = Path(data_dir) / \"mapping.json\"\n\t\tmapping = json.load(mapping_path.open())\n\t\tself.speaker2id = mapping[\"speaker2id\"]\n\t\n\t\t# Load metadata of training data.\n\t\tmetadata_path = Path(data_dir) / \"metadata.json\"\n\t\tmetadata = json.load(open(metadata_path))[\"speakers\"]\n\t\n\t\t# Get the total number of speaker.\n\t\tself.speaker_num = len(metadata.keys())\n\t\tself.data = []\n\t\tfor speaker in metadata.keys():\n\t\t\tfor utterances in metadata[speaker]:\n\t\t\t\tself.data.append([utterances[\"feature_path\"], self.speaker2id[speaker]])\n \n\tdef __len__(self):\n\t\treturn len(self.data)\n \n\tdef __getitem__(self, index):\n\t\tfeat_path, speaker = self.data[index]\n\t\t# Load preprocessed mel-spectrogram.\n\t\tmel = torch.load(os.path.join(self.data_dir, feat_path))\n\n\t\t# Segmemt mel-spectrogram into \"segment_len\" frames.\n\t\tif len(mel) > self.segment_len:\n\t\t\t# Randomly get the starting point of the segment.\n\t\t\tstart = random.randint(0, len(mel) - self.segment_len)\n\t\t\t# Get a segment with \"segment_len\" frames.\n\t\t\tmel = torch.FloatTensor(mel[start:start+self.segment_len])\n\t\telse:\n\t\t\tmel = torch.FloatTensor(mel)\n\t\t# Turn the speaker id into long for computing loss later.\n\t\tspeaker = torch.FloatTensor([speaker]).long()\n\t\treturn mel, speaker\n \n\tdef get_speaker_number(self):\n\t\treturn self.speaker_num","metadata":{"id":"KpuGxl4CI2pr","execution":{"iopub.status.busy":"2022-10-12T07:24:11.212106Z","iopub.execute_input":"2022-10-12T07:24:11.212851Z","iopub.status.idle":"2022-10-12T07:24:11.225708Z","shell.execute_reply.started":"2022-10-12T07:24:11.212811Z","shell.execute_reply":"2022-10-12T07:24:11.224731Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"## Dataloader\n- Split dataset into training dataset(90%) and validation dataset(10%).\n- Create dataloader to iterate the data.","metadata":{"id":"668hverTMlGN"}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.nn.utils.rnn import pad_sequence\n\n\ndef collate_batch(batch):\n\t# Process features within a batch.\n\t\"\"\"Collate a batch of data.\"\"\"\n\tmel, speaker = zip(*batch)\n\t# Because we train the model batch by batch, we need to pad the features in the same batch to make their lengths the same.\n\tmel = pad_sequence(mel, batch_first=True, padding_value=-20)    # pad log 10^(-20) which is very small value.\n\t# mel: (batch size, length, 40)\n\treturn mel, torch.FloatTensor(speaker).long()\n\n\ndef get_dataloader(data_dir, batch_size, n_workers):\n\t\"\"\"Generate dataloader\"\"\"\n\tdataset = myDataset(data_dir)\n\tspeaker_num = dataset.get_speaker_number()\n\t# Split dataset into training dataset and validation dataset\n\ttrainlen = int(0.9 * len(dataset))\n\tlengths = [trainlen, len(dataset) - trainlen]\n\ttrainset, validset = random_split(dataset, lengths)\n\n\ttrain_loader = DataLoader(\n\t\ttrainset,\n\t\tbatch_size=batch_size,\n\t\tshuffle=True,\n\t\tdrop_last=True,\n\t\tnum_workers=n_workers,\n\t\tpin_memory=True,\n\t\tcollate_fn=collate_batch,\n\t)\n\tvalid_loader = DataLoader(\n\t\tvalidset,\n\t\tbatch_size=batch_size,\n\t\tnum_workers=n_workers,\n\t\tdrop_last=True,\n\t\tpin_memory=True,\n\t\tcollate_fn=collate_batch,\n\t)\n\n\treturn train_loader, valid_loader, speaker_num","metadata":{"id":"B7c2gZYoJDRS","execution":{"iopub.status.busy":"2022-10-12T07:24:11.228746Z","iopub.execute_input":"2022-10-12T07:24:11.229186Z","iopub.status.idle":"2022-10-12T07:24:11.241459Z","shell.execute_reply.started":"2022-10-12T07:24:11.229148Z","shell.execute_reply":"2022-10-12T07:24:11.240494Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"# Model\n- TransformerEncoderLayer:\n  - Base transformer encoder layer in [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n  - Parameters:\n    - d_model: the number of expected features of the input (required).\n\n    - nhead: the number of heads of the multiheadattention models (required).\n\n    - dim_feedforward: the dimension of the feedforward network model (default=2048).\n\n    - dropout: the dropout value (default=0.1).\n\n    - activation: the activation function of intermediate layer, relu or gelu (default=relu).\n\n- TransformerEncoder:\n  - TransformerEncoder is a stack of N transformer encoder layers\n  - Parameters:\n    - encoder_layer: an instance of the TransformerEncoderLayer() class (required).\n\n    - num_layers: the number of sub-encoder-layers in the encoder (required).\n\n    - norm: the layer normalization component (optional).","metadata":{"id":"5FOSZYxrMqhc"}},{"cell_type":"code","source":"!pip install conformer","metadata":{"execution":{"iopub.status.busy":"2022-10-12T07:24:11.242629Z","iopub.execute_input":"2022-10-12T07:24:11.242919Z","iopub.status.idle":"2022-10-12T07:24:20.886804Z","shell.execute_reply.started":"2022-10-12T07:24:11.242883Z","shell.execute_reply":"2022-10-12T07:24:20.885487Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Requirement already satisfied: conformer in /opt/conda/lib/python3.7/site-packages (0.2.5)\nRequirement already satisfied: einops in /opt/conda/lib/python3.7/site-packages (from conformer) (0.5.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from conformer) (1.11.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->conformer) (4.3.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom conformer import ConformerBlock\n\nclass self_Attentive_pooling(nn.Module):\n    def __init__(self, dim):\n        super(self_Attentive_pooling, self).__init__()\n        self.sap_linear = nn.Linear(dim, dim)\n        self.attention = nn.Parameter(torch.FloatTensor(dim,1))\n        torch.nn.init.normal_(self.attention, std=.02)\n\n    def forward(self, x):\n        h=torch.tanh(self.sap_linear(x))\n        w=torch.matmul(h, self.attention).squeeze(dim=2)\n        w=F.softmax(w, dim=1).view(x.size(0), x.size(1), 1)\n        x=torch.sum(x * w, dim=1)\n        return x\n\nclass AMSoftmax(nn.Module):\n    def __init__(self, in_feats, n_classes, m=0.3, s=15, annealing=False):\n        super(AMSoftmax, self).__init__()\n        self.linear = nn.Linear(in_feats, n_classes, bias=False)\n        self.m = m\n        self.s = s\n\n    def _am_logsumexp(self, logits):\n        max_x = torch.max(logits, dim=-1)[0].unsqueeze(-1)\n        term1 = (self.s*(logits - (max_x + self.m))).exp()\n        term2 = (self.s * (logits - max_x)).exp().sum(-1).unsqueeze(-1) - (self.s*(logits-max_x)).exp()\n        return self.s * max_x + (term2 + term1).log()\n\n    def forward(self, *inputs):\n        x_vector = F.normalize(inputs[0], p=2, dim=-1)\n        self.linear.weight.data = F.normalize(self.linear.weight.data, p=2, dim=-1)\n        logits = self.linear(x_vector)\n        scaled_logits = (logits-self.m)*self.s\n        return scaled_logits - self._am_logsumexp(logits)\n\nclass Classifier(nn.Module):\n\tdef __init__(self, d_model=256, n_spks=600, dropout=0.3):\n\t\tsuper().__init__()\n\t\t# Project the dimension of features from that of input into d_model.\n\t\tself.prenet = nn.Linear(40, d_model)\n\t\t# TODO:\n\t\t#   Change Transformer to Conformer.\n\t\t#   https://arxiv.org/abs/2005.08100\n# \t\tself.encoder_layer = nn.TransformerEncoderLayer(\n# \t\t\td_model=d_model, dim_feedforward=1024, nhead=2, dropout=dropout\n# \t\t)\n# \t\tself.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=3)\n\t\tself.blocks = nn.ModuleList([\n            ConformerBlock(\n                dim=d_model,\n                dim_head=64,\n                heads=8,\n                ff_mult=4,\n                conv_expansion_factor=2,\n                conv_kernel_size=15,\n                attn_dropout=dropout,\n                ff_dropout=dropout,\n                conv_dropout=dropout\n            )\n            for i in range(6)])\n\t\t# Project the the dimension of features from d_model into speaker nums.\n\t\tself.sap = self_Attentive_pooling(d_model)\n\t\tself.pred_layer = AMSoftmax(d_model, n_spks)\n\n\tdef forward(self, mels):\n\t\t\"\"\"\n\t\targs:\n\t\t\tmels: (batch size, length, 40)\n\t\treturn:\n\t\t\tout: (batch size, n_spks)\n\t\t\"\"\"\n\t\t# out: (batch size, length, d_model)\n\t\tout = self.prenet(mels)\n\t\t# out: (length, batch size, d_model)\n# \t\tout = out.permute(1, 0, 2)\n\t\t# The encoder layer expect features in the shape of (length, batch size, d_model).\n# \t\tout = self.encoder(out)\n\t\t# out: (batch size, length, d_model)\n# \t\tout = out.transpose(0, 1)\n\t\t# mean pooling\n# \t\tstats = self.sap(out)\n\t\tfor blk in self.blocks:\n\t\t\tout = blk(out)\n\t\tstats = self.sap(out)\n\t\t# out: (batch, n_spks)\n\t\tout = self.pred_layer(stats)\n\t\treturn out","metadata":{"id":"iXZ5B0EKJGs8","execution":{"iopub.status.busy":"2022-10-12T07:24:20.889013Z","iopub.execute_input":"2022-10-12T07:24:20.889545Z","iopub.status.idle":"2022-10-12T07:24:20.909804Z","shell.execute_reply.started":"2022-10-12T07:24:20.889501Z","shell.execute_reply":"2022-10-12T07:24:20.908744Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"# Learning rate schedule\n- For transformer architecture, the design of learning rate schedule is different from that of CNN.\n- Previous works show that the warmup of learning rate is useful for training models with transformer architectures.\n- The warmup schedule\n  - Set learning rate to 0 in the beginning.\n  - The learning rate increases linearly from 0 to initial learning rate during warmup period.","metadata":{"id":"W7yX8JinM5Ly"}},{"cell_type":"code","source":"import math\n\nimport torch\nfrom torch.optim import Optimizer\nfrom torch.optim.lr_scheduler import LambdaLR\n\n\ndef get_cosine_schedule_with_warmup(\n\toptimizer: Optimizer,\n\tnum_warmup_steps: int,\n\tnum_training_steps: int,\n\tnum_cycles: float = 0.5,\n\tlast_epoch: int = -1,\n):\n\t\"\"\"\n\tCreate a schedule with a learning rate that decreases following the values of the cosine function between the\n\tinitial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the\n\tinitial lr set in the optimizer.\n\n\tArgs:\n\t\toptimizer (:class:`~torch.optim.Optimizer`):\n\t\tThe optimizer for which to schedule the learning rate.\n\t\tnum_warmup_steps (:obj:`int`):\n\t\tThe number of steps for the warmup phase.\n\t\tnum_training_steps (:obj:`int`):\n\t\tThe total number of training steps.\n\t\tnum_cycles (:obj:`float`, `optional`, defaults to 0.5):\n\t\tThe number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0\n\t\tfollowing a half-cosine).\n\t\tlast_epoch (:obj:`int`, `optional`, defaults to -1):\n\t\tThe index of the last epoch when resuming training.\n\n\tReturn:\n\t\t:obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n\t\"\"\"\n\tdef lr_lambda(current_step):\n\t\t# Warmup\n\t\tif current_step < num_warmup_steps:\n\t\t\treturn float(current_step) / float(max(1, num_warmup_steps))\n\t\t# decadence\n\t\tprogress = float(current_step - num_warmup_steps) / float(\n\t\t\tmax(1, num_training_steps - num_warmup_steps)\n\t\t)\n\t\treturn max(\n\t\t\t0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))\n\t\t)\n\n\treturn LambdaLR(optimizer, lr_lambda, last_epoch)","metadata":{"id":"ykt0N1nVJJi2","execution":{"iopub.status.busy":"2022-10-12T07:24:20.910932Z","iopub.execute_input":"2022-10-12T07:24:20.912234Z","iopub.status.idle":"2022-10-12T07:24:20.927590Z","shell.execute_reply.started":"2022-10-12T07:24:20.912197Z","shell.execute_reply":"2022-10-12T07:24:20.926511Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"# Model Function\n- Model forward function.","metadata":{"id":"-LN2XkteM_uH"}},{"cell_type":"code","source":"import torch\n\n\ndef model_fn(batch, model, criterion, device):\n\t\"\"\"Forward a batch through the model.\"\"\"\n\n\tmels, labels = batch\n\tmels = mels.to(device)\n\tlabels = labels.to(device)\n\n\touts = model(mels)\n\n\tloss = criterion(outs, labels)\n\n\t# Get the speaker id with highest probability.\n\tpreds = outs.argmax(1)\n\t# Compute accuracy.\n\taccuracy = torch.mean((preds == labels).float())\n\n\treturn loss, accuracy","metadata":{"id":"N-rr8529JMz0","execution":{"iopub.status.busy":"2022-10-12T07:24:20.929011Z","iopub.execute_input":"2022-10-12T07:24:20.929418Z","iopub.status.idle":"2022-10-12T07:24:20.942056Z","shell.execute_reply.started":"2022-10-12T07:24:20.929384Z","shell.execute_reply":"2022-10-12T07:24:20.941045Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"# Validate\n- Calculate accuracy of the validation set.","metadata":{"id":"cwM_xyOtNCI2"}},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\n\n\ndef valid(dataloader, model, criterion, device): \n\t\"\"\"Validate on validation set.\"\"\"\n\n\tmodel.eval()\n\trunning_loss = 0.0\n\trunning_accuracy = 0.0\n\tpbar = tqdm(total=len(dataloader.dataset), ncols=0, desc=\"Valid\", unit=\" uttr\")\n\n\tfor i, batch in enumerate(dataloader):\n\t\twith torch.no_grad():\n\t\t\tloss, accuracy = model_fn(batch, model, criterion, device)\n\t\t\trunning_loss += loss.item()\n\t\t\trunning_accuracy += accuracy.item()\n\n\t\tpbar.update(dataloader.batch_size)\n\t\tpbar.set_postfix(\n\t\t\tloss=f\"{running_loss / (i+1):.2f}\",\n\t\t\taccuracy=f\"{running_accuracy / (i+1):.2f}\",\n\t\t)\n\n\tpbar.close()\n\tmodel.train()\n\n\treturn running_accuracy / len(dataloader)","metadata":{"id":"YAiv6kpdJRTJ","execution":{"iopub.status.busy":"2022-10-12T07:24:20.944995Z","iopub.execute_input":"2022-10-12T07:24:20.945416Z","iopub.status.idle":"2022-10-12T07:24:20.955295Z","shell.execute_reply.started":"2022-10-12T07:24:20.945390Z","shell.execute_reply":"2022-10-12T07:24:20.954374Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"# Main function","metadata":{"id":"g6ne9G-eNEdG"}},{"cell_type":"code","source":"from tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, random_split\n\n\ndef parse_args():\n\t\"\"\"arguments\"\"\"\n\tconfig = {\n\t\t\"data_dir\": \"../input/ml2022spring-hw4/Dataset\",\n\t\t\"save_path\": \"model.ckpt\",\n\t\t\"batch_size\": 128,\n\t\t\"n_workers\": 8,\n\t\t\"valid_steps\": 2000,\n\t\t\"warmup_steps\": 1000,\n\t\t\"save_steps\": 10000,\n\t\t\"total_steps\": 25000,\n\t}\n\n\treturn config\n\n\ndef main(\n\tdata_dir,\n\tsave_path,\n\tbatch_size,\n\tn_workers,\n\tvalid_steps,\n\twarmup_steps,\n\ttotal_steps,\n\tsave_steps,\n):\n\t\"\"\"Main function.\"\"\"\n\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\tprint(f\"[Info]: Use {device} now!\")\n\n\ttrain_loader, valid_loader, speaker_num = get_dataloader(data_dir, batch_size, n_workers)\n\ttrain_iterator = iter(train_loader)\n\tprint(f\"[Info]: Finish loading data!\",flush = True)\n\n\tmodel = Classifier(n_spks=speaker_num).to(device)\n\tcriterion = nn.CrossEntropyLoss()\n# \tcriterion = AMSoftmax()\n\toptimizer = AdamW(model.parameters(), lr=1e-3)\n\tscheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n\tprint(f\"[Info]: Finish creating model!\",flush = True)\n\n\tbest_accuracy = -1.0\n\tbest_state_dict = None\n\n\tpbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n\n\tfor step in range(total_steps):\n\t\t# Get data\n\t\ttry:\n\t\t\tbatch = next(train_iterator)\n\t\texcept StopIteration:\n\t\t\ttrain_iterator = iter(train_loader)\n\t\t\tbatch = next(train_iterator)\n\n\t\tloss, accuracy = model_fn(batch, model, criterion, device)\n\t\tbatch_loss = loss.item()\n\t\tbatch_accuracy = accuracy.item()\n\n\t\t# Updata model\n\t\tloss.backward()\n\t\toptimizer.step()\n\t\tscheduler.step()\n\t\toptimizer.zero_grad()\n\n\t\t# Log\n\t\tpbar.update()\n\t\tpbar.set_postfix(\n\t\t\tloss=f\"{batch_loss:.2f}\",\n\t\t\taccuracy=f\"{batch_accuracy:.2f}\",\n\t\t\tstep=step + 1,\n\t\t)\n\n\t\t# Do validation\n\t\tif (step + 1) % valid_steps == 0:\n\t\t\tpbar.close()\n\n\t\t\tvalid_accuracy = valid(valid_loader, model, criterion, device)\n\n\t\t\t# keep the best model\n\t\t\tif valid_accuracy > best_accuracy:\n\t\t\t\tbest_accuracy = valid_accuracy\n\t\t\t\tbest_state_dict = model.state_dict()\n\n\t\t\tpbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n\n\t\t# Save the best model so far.\n\t\tif (step + 1) % save_steps == 0 and best_state_dict is not None:\n\t\t\ttorch.save(best_state_dict, save_path)\n\t\t\tpbar.write(f\"Step {step + 1}, best model saved. (accuracy={best_accuracy:.4f})\")\n\n\tpbar.close()\n\n\nif __name__ == \"__main__\":\n\tmain(**parse_args())","metadata":{"id":"Usv9s-CuJSG7","outputId":"0e4eb5c2-901f-419e-b917-61860e81c8ae","colab":{"base_uri":"https://localhost:8080/","height":375},"execution":{"iopub.status.busy":"2022-10-12T07:24:20.959703Z","iopub.execute_input":"2022-10-12T07:24:20.960505Z","iopub.status.idle":"2022-10-12T07:24:32.460955Z","shell.execute_reply.started":"2022-10-12T07:24:20.960461Z","shell.execute_reply":"2022-10-12T07:24:32.459638Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"[Info]: Use cuda now!\n[Info]: Finish loading data!\n[Info]: Finish creating model!\n","output_type":"stream"},{"name":"stderr","text":"\n\nTrain:   0% 0/2000 [00:00<?, ? step/s]\u001b[A\u001b[A\n\nTrain:   0% 1/2000 [00:00<30:59,  1.08 step/s]\u001b[A\u001b[A\n\nTrain:   0% 1/2000 [00:00<30:59,  1.08 step/s, accuracy=0.00, loss=6.86, step=1]\u001b[A\u001b[A\n\nTrain:   0% 2/2000 [00:01<20:16,  1.64 step/s, accuracy=0.00, loss=6.86, step=1]\u001b[A\u001b[A\n\nTrain:   0% 2/2000 [00:01<20:16,  1.64 step/s, accuracy=0.00, loss=6.88, step=2]\u001b[A\u001b[A\n\nTrain:   0% 3/2000 [00:01<16:54,  1.97 step/s, accuracy=0.00, loss=6.88, step=2]\u001b[A\u001b[A\n\nTrain:   0% 3/2000 [00:01<16:54,  1.97 step/s, accuracy=0.00, loss=6.80, step=3]\u001b[A\u001b[A\n\nTrain:   0% 4/2000 [00:02<15:18,  2.17 step/s, accuracy=0.00, loss=6.80, step=3]\u001b[A\u001b[A\n\nTrain:   0% 4/2000 [00:02<15:18,  2.17 step/s, accuracy=0.02, loss=6.82, step=4]\u001b[A\u001b[A\n\nTrain:   0% 5/2000 [00:02<14:23,  2.31 step/s, accuracy=0.02, loss=6.82, step=4]\u001b[A\u001b[A\n\nTrain:   0% 5/2000 [00:02<14:23,  2.31 step/s, accuracy=0.00, loss=6.86, step=5]\u001b[A\u001b[A\n\nTrain:   0% 6/2000 [00:02<13:51,  2.40 step/s, accuracy=0.00, loss=6.86, step=5]\u001b[A\u001b[A\n\nTrain:   0% 6/2000 [00:02<13:51,  2.40 step/s, accuracy=0.00, loss=6.97, step=6]\u001b[A\u001b[A\n\nTrain:   0% 7/2000 [00:03<13:28,  2.46 step/s, accuracy=0.00, loss=6.97, step=6]\u001b[A\u001b[A\n\nTrain:   0% 7/2000 [00:03<13:28,  2.46 step/s, accuracy=0.01, loss=6.75, step=7]\u001b[A\u001b[A\n\nTrain:   0% 8/2000 [00:03<13:13,  2.51 step/s, accuracy=0.01, loss=6.75, step=7]\u001b[A\u001b[A\n\nTrain:   0% 8/2000 [00:03<13:13,  2.51 step/s, accuracy=0.00, loss=6.90, step=8]\u001b[A\u001b[A\n\nTrain:   0% 9/2000 [00:04<13:03,  2.54 step/s, accuracy=0.00, loss=6.90, step=8]\u001b[A\u001b[A\n\nTrain:   0% 9/2000 [00:04<13:03,  2.54 step/s, accuracy=0.00, loss=6.80, step=9]\u001b[A\u001b[A\n\nTrain:   0% 10/2000 [00:04<12:56,  2.56 step/s, accuracy=0.00, loss=6.80, step=9]\u001b[A\u001b[A\n\nTrain:   0% 10/2000 [00:04<12:56,  2.56 step/s, accuracy=0.00, loss=6.89, step=10]\u001b[A\u001b[A\n\nTrain:   1% 11/2000 [00:04<12:53,  2.57 step/s, accuracy=0.00, loss=6.89, step=10]\u001b[A\u001b[A\n\nTrain:   1% 11/2000 [00:04<12:53,  2.57 step/s, accuracy=0.00, loss=6.75, step=11]\u001b[A\u001b[A\n\nTrain:   1% 12/2000 [00:05<12:47,  2.59 step/s, accuracy=0.00, loss=6.75, step=11]\u001b[A\u001b[A\n\nTrain:   1% 12/2000 [00:05<12:47,  2.59 step/s, accuracy=0.00, loss=6.80, step=12]\u001b[A\u001b[A\n\nTrain:   1% 13/2000 [00:05<12:44,  2.60 step/s, accuracy=0.00, loss=6.80, step=12]\u001b[A\u001b[A\n\nTrain:   1% 13/2000 [00:05<12:44,  2.60 step/s, accuracy=0.00, loss=6.69, step=13]\u001b[A\u001b[A\n\nTrain:   1% 14/2000 [00:05<12:43,  2.60 step/s, accuracy=0.00, loss=6.69, step=13]\u001b[A\u001b[A\n\nTrain:   1% 14/2000 [00:05<12:43,  2.60 step/s, accuracy=0.00, loss=6.73, step=14]\u001b[A\u001b[A\n\nTrain:   1% 15/2000 [00:06<12:42,  2.60 step/s, accuracy=0.00, loss=6.73, step=14]\u001b[A\u001b[A\n\nTrain:   1% 15/2000 [00:06<12:42,  2.60 step/s, accuracy=0.00, loss=6.93, step=15]\u001b[A\u001b[A\n\nTrain:   1% 16/2000 [00:06<12:44,  2.60 step/s, accuracy=0.00, loss=6.93, step=15]\u001b[A\u001b[A\n\nTrain:   1% 16/2000 [00:06<12:44,  2.60 step/s, accuracy=0.01, loss=6.82, step=16]\u001b[A\u001b[A\n\nTrain:   1% 17/2000 [00:07<12:42,  2.60 step/s, accuracy=0.01, loss=6.82, step=16]\u001b[A\u001b[A\n\nTrain:   1% 17/2000 [00:07<12:42,  2.60 step/s, accuracy=0.00, loss=6.85, step=17]\u001b[A\u001b[A\n\nTrain:   1% 18/2000 [00:07<12:40,  2.61 step/s, accuracy=0.00, loss=6.85, step=17]\u001b[A\u001b[A\n\nTrain:   1% 18/2000 [00:07<12:40,  2.61 step/s, accuracy=0.00, loss=6.73, step=18]\u001b[A\u001b[A\n\nTrain:   1% 19/2000 [00:07<12:39,  2.61 step/s, accuracy=0.00, loss=6.73, step=18]\u001b[A\u001b[A\n\nTrain:   1% 19/2000 [00:07<12:39,  2.61 step/s, accuracy=0.00, loss=6.84, step=19]\u001b[A\u001b[A\n\nTrain:   1% 20/2000 [00:08<12:38,  2.61 step/s, accuracy=0.00, loss=6.84, step=19]\u001b[A\u001b[A\n\nTrain:   1% 20/2000 [00:08<12:38,  2.61 step/s, accuracy=0.01, loss=6.79, step=20]\u001b[A\u001b[A\n\nTrain:   1% 21/2000 [00:08<12:36,  2.62 step/s, accuracy=0.01, loss=6.79, step=20]\u001b[A\u001b[A\n\nTrain:   1% 21/2000 [00:08<12:36,  2.62 step/s, accuracy=0.00, loss=6.67, step=21]\u001b[A\u001b[A\n\nTrain:   1% 22/2000 [00:08<12:35,  2.62 step/s, accuracy=0.00, loss=6.67, step=21]\u001b[A\u001b[A\n\nTrain:   1% 22/2000 [00:08<12:35,  2.62 step/s, accuracy=0.00, loss=6.75, step=22]\u001b[A\u001b[A\n\nTrain:   1% 23/2000 [00:09<12:35,  2.62 step/s, accuracy=0.00, loss=6.75, step=22]\u001b[A\u001b[A\n\nTrain:   1% 23/2000 [00:09<12:35,  2.62 step/s, accuracy=0.01, loss=6.55, step=23]\u001b[A\u001b[A\n\nTrain:   1% 24/2000 [00:09<12:36,  2.61 step/s, accuracy=0.01, loss=6.55, step=23]\u001b[A\u001b[A\n\nTrain:   1% 24/2000 [00:09<12:36,  2.61 step/s, accuracy=0.00, loss=6.66, step=24]\u001b[A\u001b[A\n\nTrain:   1% 25/2000 [00:10<12:35,  2.62 step/s, accuracy=0.00, loss=6.66, step=24]\u001b[A\u001b[A\n\nTrain:   1% 25/2000 [00:10<12:35,  2.62 step/s, accuracy=0.00, loss=6.49, step=25]\u001b[A\u001b[A","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/325961183.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_17/325961183.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(data_dir, save_path, batch_size, n_workers, valid_steps, warmup_steps, total_steps, save_steps)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;31m# Updata model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"# Inference\n\n## Dataset of inference","metadata":{"id":"NLatBYAhNNMx"}},{"cell_type":"code","source":"import os\nimport json\nimport torch\nfrom pathlib import Path\nfrom torch.utils.data import Dataset\n\n\nclass InferenceDataset(Dataset):\n\tdef __init__(self, data_dir):\n\t\ttestdata_path = Path(data_dir) / \"testdata.json\"\n\t\tmetadata = json.load(testdata_path.open())\n\t\tself.data_dir = data_dir\n\t\tself.data = metadata[\"utterances\"]\n\n\tdef __len__(self):\n\t\treturn len(self.data)\n\n\tdef __getitem__(self, index):\n\t\tutterance = self.data[index]\n\t\tfeat_path = utterance[\"feature_path\"]\n\t\tmel = torch.load(os.path.join(self.data_dir, feat_path))\n\n\t\treturn feat_path, mel\n\n\ndef inference_collate_batch(batch):\n\t\"\"\"Collate a batch of data.\"\"\"\n\tfeat_paths, mels = zip(*batch)\n\n\treturn feat_paths, torch.stack(mels)","metadata":{"id":"efS4pCmAJXJH","execution":{"iopub.status.busy":"2022-10-12T07:24:32.462272Z","iopub.status.idle":"2022-10-12T07:24:32.464109Z","shell.execute_reply.started":"2022-10-12T07:24:32.463836Z","shell.execute_reply":"2022-10-12T07:24:32.463865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Main funcrion of Inference","metadata":{"id":"tl0WnYwxNK_S"}},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"import json\nimport csv\nfrom pathlib import Path\nfrom tqdm.notebook import tqdm\n\nimport torch\nfrom torch.utils.data import DataLoader\n\ndef parse_args():\n\t\"\"\"arguments\"\"\"\n\tconfig = {\n\t\t\"data_dir\": \"../input/ml2022spring-hw4/Dataset\",\n\t\t\"model_path\": \"./model.ckpt\",\n\t\t\"output_path\": \"./output.csv\",\n\t}\n\n\treturn config\n\n\ndef main(\n\tdata_dir,\n\tmodel_path,\n\toutput_path,\n):\n\t\"\"\"Main function.\"\"\"\n\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\tprint(f\"[Info]: Use {device} now!\")\n\n\tmapping_path = Path(data_dir) / \"mapping.json\"\n\tmapping = json.load(mapping_path.open())\n\n\tdataset = InferenceDataset(data_dir)\n\tdataloader = DataLoader(\n\t\tdataset,\n\t\tbatch_size=1,\n\t\tshuffle=False,\n\t\tdrop_last=False,\n\t\tnum_workers=8,\n\t\tcollate_fn=inference_collate_batch,\n\t)\n\tprint(f\"[Info]: Finish loading data!\",flush = True)\n\n\tspeaker_num = len(mapping[\"id2speaker\"])\n\tmodel = Classifier(n_spks=speaker_num).to(device)\n\tmodel.load_state_dict(torch.load(model_path))\n\tmodel.eval()\n\tprint(f\"[Info]: Finish creating model!\",flush = True)\n\n\tresults = [[\"Id\", \"Category\"]]\n\tfor feat_paths, mels in tqdm(dataloader):\n\t\twith torch.no_grad():\n\t\t\tmels = mels.to(device)\n\t\t\touts = model(mels)\n\t\t\tpreds = outs.argmax(1).cpu().numpy()\n\t\t\tfor feat_path, pred in zip(feat_paths, preds):\n\t\t\t\tresults.append([feat_path, mapping[\"id2speaker\"][str(pred)]])\n\n\twith open(output_path, 'w', newline='') as csvfile:\n\t\twriter = csv.writer(csvfile)\n\t\twriter.writerows(results)\n\n\nif __name__ == \"__main__\":\n\tmain(**parse_args())","metadata":{"id":"i8SAbuXEJb2A","execution":{"iopub.status.busy":"2022-10-12T07:24:32.465657Z","iopub.status.idle":"2022-10-12T07:24:32.466465Z","shell.execute_reply.started":"2022-10-12T07:24:32.466203Z","shell.execute_reply":"2022-10-12T07:24:32.466228Z"},"trusted":true},"execution_count":null,"outputs":[]}]}